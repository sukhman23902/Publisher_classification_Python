# -*- coding: utf-8 -*-
"""Cleaning_domains.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_677IgS8W5jKC-_ySU588nUwdyn0EhLD
"""

import pandas as pd
import re
from urllib.parse import urlparse

def extract_root_domain(domain):
    """Extract the root domain from a URL or path."""
    # Remove protocol (http://, https://) and paths
    domain = re.sub(r"^https?://", "", domain)
    domain = re.sub(r"/.*$", "", domain)  # Remove paths
    domain = re.sub(r"\?.*$", "", domain)  # Remove query parameters
    domain = re.sub(r"#.*$", "", domain)  # Remove fragments
    return domain.strip().lower()

def is_valid_domain(domain):
    """Check if a domain is valid."""
    # Regex to validate domain structure
    regex = r"^([a-z0-9]+(-[a-z0-9]+)*\.)+[a-z]{2,}$"
    return re.match(regex, domain) is not None

def clean_and_validate_domains(file_path):
    """Clean and validate domains in the file."""
    # Load the file
    df = pd.read_csv(file_path)

    # Extract root domains
    df["Root Domain"] = df["Website"].apply(extract_root_domain)

    # Validate domains
    df["Is Valid"] = df["Root Domain"].apply(is_valid_domain)

    # Identify duplicates
    df["Is Duplicate"] = df["Root Domain"].duplicated(keep=False)

    # Save the cleaned and validated data to a new file
    output_file = "cleaned_domains.csv"
    df.to_csv(output_file, index=False)

    print(f"Cleaned and validated domains saved to {output_file}")
    return df

# Run the validation
file_path = "deep_input_site_status - Sheet1.csv"  # Replace with your file path
cleaned_df = clean_and_validate_domains(file_path)

# Display invalid and duplicate domains
invalid_domains = cleaned_df[~cleaned_df["Is Valid"]]
duplicate_domains = cleaned_df[cleaned_df["Is Duplicate"]]

print("\nInvalid Domains:")
print(invalid_domains[["Website", "Root Domain"]])

print("\nDuplicate Domains:")
print(duplicate_domains[["Website", "Root Domain"]])